# -*- coding: utf-8 -*-
"""minorproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SQrKuzwaOxepWwqrEpXD0Ge-w_nlc38F
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns


"""**Preprocessing**"""

# Creating the reverse dictionary to rename columns back to their original names
rename_dict = {
    'DATE	': 'Date',
    'NET SUPPLY TO GR. MUMBAI': 'Net_supply'
}
# Renaming the columns back to their original names
df.rename(columns=rename_dict, inplace=True)

df.head()  # Display the first few rows of the DataFrame

df.describe()

df.shape

#check the number of missing values
df.isnull().sum()

"""**Handle Missing values**"""

# Drop rows with null values
df.dropna(inplace=True)

# Verify that null values have been removed
print(df.isnull().sum())

"""**Visualisation of Raw Data**"""

# Using plotly.express
import plotly.express as px #import plotly library for plots
fig = px.line(df, x='DATE', y='Net_supply',labels={
                     "Date": "DATE",
                     "Net_supply": "Net_supply"

                 })
fig.show()

import tensorflow as tf
import sklearn

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,  GRU
from tensorflow.keras.layers import Dense, Dropout

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

#from datetime import datetime
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from keras.layers import LeakyReLU

"""**LSTM model**"""

!pip install keras

!pip install --upgrade tensorflow

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Apply min-max normalization to the 'Count' column
df['Normalized_Count'] = scaler.fit_transform(df[['Net_supply']])
df['Normalized_Count']

# Create arrays for features and target
x1 = df.index.values
y1 = df['Normalized_Count'].values

# Calculate the split indices
train_size1 = 1000
val_size1 = 170
test_size1 = 170

# Split the dataset
x_train1, y_train1 = x1[:train_size1], y1[:train_size1]  # Training data
x_val1, y_val1 = x1[train_size1:train_size1 + val_size1], y1[train_size1:train_size1 + val_size1]  # Validation data
x_test1, y_test1 = x1[train_size1 + val_size1:], y1[train_size1 + val_size1:]  # Testing data

# Display the split counts
print(f'Training set size: {len(x_train1)}')
print(f'Validation set size: {len(x_val1)}')
print(f'Testing set size: {len(x_test1)}')

# Create a supervised learning dataset from the time series
def create_dataset(data, time_steps=1):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:(i + time_steps)])
        y.append(data[i + time_steps])
    return np.array(X), np.array(y)

# Example parameters
time_steps = 10

# Create the dataset
x_train1, y_train1 = create_dataset(y_train1, time_steps)
x_val1, y_val1= create_dataset(y_val1, time_steps)
x_test1, y_test1 = create_dataset(y_test1, time_steps)

# Reshape the data for LSTM [samples, time_steps, features]
x_train1 = np.reshape(x_train1, (x_train1.shape[0], time_steps, 1))
x_val1 = np.reshape(x_val1, (x_val1.shape[0], time_steps, 1))
x_test1 = np.reshape(x_test1, (x_test1.shape[0], time_steps, 1))

# Define the LSTM model
model1 = Sequential()
model1.add(LSTM(50, return_sequences=True, input_shape=(time_steps, 1)))
model1.add(LSTM(50))
model1.add(Dense(1))
model1.summary()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense
from tensorflow.keras.losses import MeanSquaredError  # Importing MeanSquaredError
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import RootMeanSquaredError # Importing Adam optimizer

import tensorflow as tf  # For tf.sqrt to compute square root

model1.compile(loss=MeanSquaredError(),optimizer=Adam(learning_rate=0.0001),metrics=[RootMeanSquaredError()])

# Fit the model
model1.fit(x_train1, y_train1, epochs=100, batch_size=32, validation_data=(x_val1, y_val1))

# Make predictions
train_predictions1 = model1.predict(x_train1)
test_predictions1 = model1.predict(x_test1)

# Inverse transform the predictions
train_predictions1 = scaler.inverse_transform(train_predictions1)
test_predictions1 = scaler.inverse_transform(test_predictions1)

# Plot predictions
plt.figure(figsize=(10, 6))

# Plot actual data
plt.plot(df.index[time_steps:], df['Net_supply'][time_steps:], label='Actual', color='blue')

# Plot training predictions
plt.plot(df.index[time_steps:time_steps+len(train_predictions1)], train_predictions1, label='Train Predictions',color='green')

# Plot testing predictions
test_pred_index1 = range(time_steps+len(train_predictions1), time_steps+len(train_predictions1)+len(test_predictions1))
plt.plot(df.index[test_pred_index1], test_predictions1, label='Test Predictions',color='orange')

plt.xlabel('Year')
plt.ylabel('Net supply')
plt.legend()
plt.show()

# Create a new column for the predictions in the DataFrame using .loc
df['lstm_predictions'] = np.nan

# Ensure the index alignment for assigning predicted values
start_index = train_size1 + val_size1 + time_steps  # Adjust this according to your needs

# Assign the predicted values to the appropriate locations in the DataFrame
df.iloc[start_index:start_index+len(test_predictions1 ), df.columns.get_loc('lstm_predictions')] = test_predictions1 .flatten()

# Display the DataFrame with predictions
print(df)

"""**prophet Model**"""

from prophet import Prophet

df1 = df.copy()
df1 = df1.drop(columns=['Normalized_Count', 'lstm_predictions'])
df1.head()

# Rename the columns to fit Prophet's requirements
df1.rename(columns={'DATE': 'ds', 'Net_supply': 'y'}, inplace=True)

# Convert the 'ds' column to datetime
df1['ds'] = pd.to_datetime(df1['ds'])

# Initialize the Prophet model
model = Prophet()

# Fit the model on your data
model.fit(df1)

# Create a DataFrame for future dates
future = model.make_future_dataframe(periods=341)

# Make predictions
forecast = model.predict(future)

# Round predictions to whole numbers and ensure no negative predictions
forecast['yhat'] = forecast['yhat'].round().clip(lower=0)
forecast['yhat_lower'] = forecast['yhat_lower'].round().clip(lower=0)
forecast['yhat_upper'] = forecast['yhat_upper'].round().clip(lower=0)

# Plot the forecast with actual data
plt.figure(figsize=(14, 7))

# Plot actual data
plt.plot(df1['ds'], df1['y'], label='Actual', color='blue')

# Plot predicted data
plt.plot(forecast['ds'], forecast['yhat'], label='Predictions', color='green')

plt.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='gray', alpha=0.2)

plt.xlabel('Date')
plt.ylabel('net supply')
plt.title('Forecast of  net supply')
plt.legend()
plt.show()

forecast

# Merge the predictions back into the original DataFrame
# Create a new DataFrame with predictions only for the forecast period
predictions_df = forecast[['ds', 'yhat']].rename(columns={'yhat': 'predicted_count'})

print("Columns in df:", df1.columns)
print("Columns in predictions_df:", predictions_df.columns)

df1= pd.merge(df1, predictions_df, on='ds', how='left')

# Fill in missing predictions for the original data period with NaN
df1['predicted_count'] = df1['predicted_count'].fillna(value=pd.Series([np.nan] * len(df)))

# Display the DataFrame with predictions
print(df1)

"""**ARIMA model**"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

df.head()

df2 = df.copy()
df2 = df2.drop(columns=['Normalized_Count', 'lstm_predictions'])
df2.head()

df2.set_index('DATE', inplace=True)

# Check ACF and PACF plots to determine parameters for SARIMA
plot_acf(df2)
plt.show()

plot_pacf(df2)
plt.show()

#TO check whether the data is stationary or not

from statsmodels.tsa.stattools import adfuller
import pandas as pd


appointments = df2['Net_supply']  # Replace with the actual column name

# Perform ADF test
result = adfuller(appointments)

print('ADF Statistic:', result[0])
print('p-value:', result[1])
for key, value in result[4].items():
    print('Critical Values:')
    print(f'   {key}, {value}')

#data is stationary

from statsmodels.tsa.arima.model import ARIMA
# Fit ARIMA model
arima_model = ARIMA(appointments, order=(1, 0, 1))
fit1 = arima_model.fit()

# Summary of the model
print(fit1.summary())

# Generate predictions for the next 341 days
predictions1 = fit1.get_forecast(steps=341)
predicted_mean1 = predictions1.predicted_mean

appointments.index = pd.to_datetime(appointments.index)

# Create a new DataFrame to store the predictions
last_date = appointments.index[-1]
future_dates = [last_date + pd.Timedelta(days=i) for i in range(1, 342)]
predictions_df1 = pd.DataFrame(predicted_mean1.values, index=future_dates, columns=['arima_predictions'])
predictions_df1.head()

# Plot predictions
plt.figure(figsize=(10, 6))

# Plot actual data
plt.plot(df2.index, df2['Net_supply'], label='Actual', color='blue')

# Plot training predictions
plt.plot(predictions_df1.index, predicted_mean1.values, label='Predictions',color='green')


plt.xlabel('Year')
plt.ylabel('supply of water')
plt.legend()
plt.show()



"""**SARIMA model**"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
# Fit SARIMA model
sarima_model = SARIMAX(appointments, order=(1, 0, 1), seasonal_order=(1, 0, 1, 7))
fit = sarima_model.fit()

# Generate predictions for the next 341 days
predictions = fit.get_forecast(steps=341)
predicted_mean = predictions.predicted_mean

# Create a new DataFrame to store the predictions
last_date = appointments.index[-1]
future_dates = [last_date + pd.Timedelta(days=i) for i in range(1, 342)]
predictions_df = pd.DataFrame(predicted_mean.values, index=future_dates, columns=['sarima_predictions'])
predictions_df.head()

appointments.index = pd.to_datetime(appointments.index)

# Ensure df2 index is a DatetimeIndex
if not pd.api.types.is_datetime64_any_dtype(df2.index):
    df2.index = pd.to_datetime(df2.index)

# Ensure predictions_df index is a DatetimeIndex
if not pd.api.types.is_datetime64_any_dtype(predictions_df.index):
    predictions_df.index = pd.to_datetime(predictions_df.index)

# Plot predictions
plt.figure(figsize=(10, 6))

# Plot actual data
plt.plot(df2.index, df2['Net_supply'], label='Actual', color='blue')

# Plot training predictions
plt.plot(predictions_df.index, predicted_mean.values, label='Predictions',color='green')


plt.xlabel('Year')
plt.ylabel('supply of water')
plt.legend()
plt.show()

"""**Comparative analysis**"""

# Assuming lstm_predictions is defined and contains the predictions from the LSTM model
lstm_predictions = df['lstm_predictions'][-30:].values  # Extract the last 30 LSTM predicted values

# Extract only the predicted values from Prophet forecast
# Assuming prophet_forecast is a DataFrame with a 'yhat' column for predictions
prophet_predictions = df1['predicted_count'][-30:].values  # Extract the last 30 predicted values

# For actual values, ensure you're only getting numeric values (no timestamps)
actual_values = df['Net_supply'][-30:].values  # Use only the last 30 numeric values for comparison

# Extract ARIMA predictions
arima_predictions = fit1.predict(start=len(df) - 30, end=len(df) - 1).values  # Ensure this returns the last 30 predicted values

# Extract SARIMA predictions
sarima_predictions = fit.predict(start=len(df) - 30, end=len(df) - 1).values  # Same for SARIMA

# Check types of the predictions and actual values
print(f"Prophet Predictions Type: {type(prophet_predictions)}")
print(f"LSTM Predictions Type: {type(lstm_predictions)}")
print(f"Arima Predictions Type: {type(arima_predictions)}")
print(f"Sarima Predictions Type: {type(sarima_predictions)}")

# Plotting the actual values and predictions for comparison
plt.figure(figsize=(12, 6))
plt.plot(actual_values, label='Actual Values', marker='o', color='black')
plt.plot(prophet_predictions, label='Prophet Predictions', marker='o', color='blue')
plt.plot(arima_predictions, label='ARIMA Predictions', marker='o', color='red')
plt.plot(sarima_predictions, label='SARIMA Predictions', marker='o', color='green')
plt.plot(lstm_predictions, label='LSTM Predictions', marker='o', color='purple')

plt.title('Actual vs Predicted Water Demand')
plt.xlabel('Time Steps (last 30)')
plt.ylabel('Water Demand')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Adding MAPE calculation
def mean_absolute_percentage_error(true_values, predicted_values):
    return np.mean(np.abs((true_values - predicted_values) / true_values)) * 100

# Updated calculate_metrics function
def calculate_metrics(true_values, predicted_values):
    mse = mean_squared_error(true_values, predicted_values)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true_values, predicted_values)
    mape = mean_absolute_percentage_error(true_values, predicted_values)
    return mse, rmse, mae, mape

# Prophet metrics
prophet_mse, prophet_rmse, prophet_mae, prophet_mape = calculate_metrics(actual_values, prophet_predictions)
print(f'Prophet - MSE: {prophet_mse:.4f}, RMSE: {prophet_rmse:.4f}, MAE: {prophet_mae:.4f}, MAPE: {prophet_mape:.4f}%')

# ARIMA metrics
arima_mse, arima_rmse, arima_mae, arima_mape = calculate_metrics(actual_values, arima_predictions)
print(f'ARIMA - MSE: {arima_mse:.4f}, RMSE: {arima_rmse:.4f}, MAE: {arima_mae:.4f}, MAPE: {arima_mape:.4f}%')

# SARIMA metrics
sarima_mse, sarima_rmse, sarima_mae, sarima_mape = calculate_metrics(actual_values, sarima_predictions)
print(f'SARIMA - MSE: {sarima_mse:.4f}, RMSE: {sarima_rmse:.4f}, MAE: {sarima_mae:.4f}, MAPE: {sarima_mape:.4f}%')

# LSTM metrics
lstm_mse, lstm_rmse, lstm_mae, lstm_mape = calculate_metrics(actual_values, lstm_predictions)
print(f'LSTM - MSE: {lstm_mse:.4f}, RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}, MAPE: {lstm_mape:.4f}%')